{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\weixi\\\\Box\\\\ds_take home\\\\Wei'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append( 'C:\\\\Users\\\\weixi\\\\Box\\\\ds_take home\\\\data\\\\DS_Challenges\\\\Conversion_Rate' )\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.metrics import auc, roc_curve, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "from sklearn import preprocessing, svm, metrics  \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import h2o\n",
    "from h2o.frame import H2OFrame\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# importlib.reload(sys.modules['evaluate_model'])\n",
    "\n",
    "# from evaluate_model import evaluate_classification,plot_confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0] load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>new_user</th>\n",
       "      <th>source</th>\n",
       "      <th>total_pages_visited</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>Seo</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Seo</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>China</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>Seo</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>Seo</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  age  new_user source  total_pages_visited  converted\n",
       "0      UK   25         1    Ads                    1          0\n",
       "1      US   23         1    Seo                    5          0\n",
       "2      US   28         1    Seo                    4          0\n",
       "3   China   39         1    Seo                    5          0\n",
       "4      US   30         1    Seo                    6          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =  pd.read_csv('C:\\\\Users\\\\weixi\\\\Box\\\\ds_take home\\\\data\\\\DS_Challenges\\\\Conversion_Rate\\\\conversion_project.csv')\n",
    "# if it has date format, use parse_date which parse data into date\n",
    "data = pd.read_csv('',parse_date =['the attributes name'])\n",
    "\n",
    "# check data information \n",
    "data.head()\n",
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Data preprocessing\n",
    "- check the missing(nan) values \n",
    "- check the outliers - extreme values for each attributes\n",
    "    - box plot/scatter plot\n",
    "    - Z score/IQR \n",
    "- check duplicates\n",
    "- profiling report \n",
    "    - from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null and fill na\n",
    "data.isnull().sum()\n",
    "data.fillna(1)\n",
    "values = {'A':0,'B':1}\n",
    "data.fillna(values) # fill na based on the dictionary specified\n",
    "\n",
    "# check unique\n",
    "len(data.unique()) == len(data)\n",
    "data_unique = dict()\n",
    "for column in data.columns:\n",
    "    data_unique[column] = sorted(data[column].unique())\n",
    "print(data_unique)\n",
    "\n",
    "# check duplicates\n",
    "data.duplicated(keep='First').sum() # num of duplicates\n",
    "data.drop_duplicates(subsets=['xx','xxx'], keep='first') # base on subsets drop duplicates\n",
    "\n",
    "# outlier\n",
    "sns.boxplot(data)\n",
    "np.where((xxx>xxx)&(xxx>xxx)) # output the index\n",
    "\n",
    "ax.scatter(xxx,xxx)\n",
    "\n",
    "# z score\n",
    "from scipy import stats\n",
    "z= np.abs(stats.zscore(XXX))\n",
    "threshold =3\n",
    "np.where(z>threshold)\n",
    "\n",
    "# IQR\n",
    "Q1 = np.percentile(xxx,25,interpolation='midpoint')\n",
    "Q3 = np.percentile(xxx, 75, interpolation='midepoint')\n",
    "IQR = Q3-Q1\n",
    "lower = Q1-1.5*IQR\n",
    "upper = Q3+1.5*IQR\n",
    "lowerindex = np.where(xxx<lower)\n",
    "upperindex = np.where(xxx>upper)\n",
    "data.drop(lowerindex[0],inplace=True) # drop from the data set\n",
    "data.drop(upperindex[0],inplace=True)\n",
    "\n",
    "# Profiling conmmand\n",
    "profile_report = ProfileReport(xxx,title='xxx',explorative=True)\n",
    "profile_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] Exploratory data analysis \n",
    "- check distributions for all varibales\n",
    "    - continous varible: sns.distplot\n",
    "    - catogorical variable: sns.barplot(x,y), sns.count(x)\n",
    "- distribution differences are between the con and non-con: imbalanced dataset\n",
    "- check every pair plots: sns.pairplot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  continouse distn/histgram plot\n",
    "fig,ax =  plt.subplots(nrows=2, ncols=1,figsize=(18,10))\n",
    "hist_kws = {'histtype':'bar','edgecolor':'black','alpha':0.2}\n",
    "sns.distplot(subset[subset['quit']==1]['salary'],label='early quit',ax=ax[0],hist_kws = hist_kws)\n",
    "sns.distplot(subset[subset['quit']==0]['salary'],label='not early quit',ax=ax[0],hist_kws = hist_kws)\n",
    "ax[0].set_title('histogram of salary')\n",
    "ax[0].set_xlabel('salary')\n",
    "ax[0].set_ylabel('frequency')\n",
    "ax[0].legend()\n",
    "\n",
    "# categorical - countplot/barplot\n",
    "fig, ax= plt.subplots(nrows=1,ncols =2, figsize=(18,6))\n",
    "sns.countplot(x='date',hue='test',data =data,ax= ax[0])\n",
    "ax[0].set_title('Count plot of date',fontsize = 14)\n",
    "\n",
    "groupby_country = data[data['test']== 0][['conversion','country']].groupby('country').mean().reset_index()\n",
    "groupby_country = groupby_country.sort_values('conversion',ascending = False)\n",
    "\n",
    "sns.barplot(x='country',y='conversion',data = groupby_country, ax = ax)\n",
    "ax[1].set_title('Mean conversion rate per date',fontsize =14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# if conversion is binary {0,1}; can plot from data directly y value means mean conversion rate for each country\n",
    "sns.barplot(x='country',y='conversion',hue='test',data=data,ax=ax)\n",
    "\n",
    "# like a PDF fig, plots mean conversion vs Age\n",
    "grouped = data[['age','converted']].groupby('age').mean().reset_index()\n",
    "\n",
    "ax[1].plot(grouped['age'],grouped['converted'],'.-')\n",
    "ax[1].set_title('Mean conversion rate vs Age')\n",
    "ax[1].set_xlabel('age')\n",
    "ax[1].set_ylabel('mean conversion rate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# check imbalance dataset\n",
    "data.groupby('label attribute').size().tolist()\n",
    "data.groupby('label attribute').size().tolist()/len(data)\n",
    "\n",
    "sns.countplot(x='',data=data)\n",
    "\n",
    "# we can also use some pariplots using sns - particular useful for continous variables\n",
    "g = sns.pairplot(data,hue='converted',size = 4 ,aspect = 0.8)\n",
    "g.fig.set_size_inches(15,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] Feature engineering and ML\n",
    "- feature engineering\n",
    "    - catogorical variable: one hot encoding/get dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinary encoder\n",
    "grade_str = X.loc[:,'tgrade'].astype(object).values[:,np.newaxis]\n",
    "grade_num = OrdinalEncoder(categories=[['I','II','III']]).fit_transform(grade_str)\n",
    "\n",
    "X_no_grade = X.drop('tgrade',axis=1)\n",
    "Xt = OneHotEncoder().fit_transform(X_no_grade)\n",
    "Xt = np.column_stack((Xt.values, grade_num))\n",
    "feature_names = X_no_grade.columns.tolist()+[\"tgrade\"]\n",
    "\n",
    "# one hot encoder - with specifying catgorical features\n",
    "feature_cat = [xxx,xxx]\n",
    "feature_con = np.diff1d(data.columns,feature_cat)\n",
    "encoder = OneHotEncoder()\n",
    "df_cat = pd.DataFrame(encoder.fit_transform(data[feature_cat]).toarray())\n",
    "data = pd.merge(data[feature_con],df_cat,left_index= True, right_index = True)\n",
    "\n",
    "# one hot encoder - not specifying catgorical features\n",
    "encoder = OneHotEncoder()\n",
    "df_all = pd.DataFrame(encoder.fit_transform(data))\n",
    "\n",
    "# get dummies\n",
    "x2 = data.loc[:,feature_cat]\n",
    "x2 = pd.get_dummies(x2,drop_first=True)# OHE those categorical features, make k-1 columns out of k\n",
    "pd.merge(x2,data[:,feature_con])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date format for time and date data\n",
    "\n",
    "# time duration - read directly or use pd.datetime()\n",
    "data = pd.read_csv('',parse_date =['the attributes name'])\n",
    "\n",
    "\n",
    "data['signup_time'] =  pd.to_datetime(data['signup_time'])\n",
    "data['purchase_time'] =  pd.to_datetime(data['purchase_time'])\n",
    "\n",
    "data['duration'] = pd.to_datetime(data['purchase_time']) - pd.to_datetime(data['signup_time'])\n",
    "data['duration'] =  data['duration'].apply(lambda x: x.seconds)\n",
    "\n",
    "# signup_day and week \n",
    "data['signup_day'] =  data['signup_time'].apply(lambda x: x.dayofweek)\n",
    "data['signup_week'] = data['signup_time'].apply(lambda x: x.week)\n",
    "# purchase day and week \n",
    "data['purchase_day'] = data['purchase_time'].apply(lambda x:x.dayofweek)\n",
    "data['purchase_week'] = data['purchase_time'].apply(lambda x:x.week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4] ML\n",
    "- train_test_split\n",
    "- cross valition for parameter optimization\n",
    "- validation /w test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y,test_y = train_test_split(x,y,test_size =0.2,random_state=0)\n",
    "\n",
    "# cv for logisticRegression\n",
    "# cv for logisticRegression\n",
    "# cv for logisticRegression\n",
    "\n",
    "Cs = [] # regularization parameters\n",
    "lr = LogisticRegressionCV(Cs = np.logspace(-3,3,7),\n",
    "                          cv =5,\n",
    "                          random_state =0,\n",
    "                         dual = False,\n",
    "                         scoring ='roc_auc',\n",
    "                         max_iter = 1000,\n",
    "                         n_jobs =-1,\n",
    "                         verbose =1)\n",
    "lr.fit(train_x,train_y)\n",
    "lr.C_ # output the best regularization parameters out of Cs\n",
    "lr.scores_[1] # output all the scores\n",
    "predict_test_y =  lr.predict(test_x) # prediction for test set\n",
    "classification_report(y_true=test_y,y_pred = predict_test_y) # the precision/recall/F1 score\n",
    "\n",
    "# feature importance based on  the coefficient values\n",
    "feature_impt = pd.DataFrame({'name':train_x.columns,'coeff':lr.coef_[0]})\n",
    "feature_impt['impt'] = np.abs(feature_impt['coeff'])\n",
    "feature_impt.sort_values(by='impt',inplace=True,ascending=False)\n",
    "feature_impt\n",
    "\n",
    "\n",
    "# cv for RandomForest\n",
    "# cv for RandomForest\n",
    "# cv for RandomForest\n",
    "\n",
    "# grid search for KEY parameters\n",
    "n_estimators =  [int(x) for x in np.linspace(100,200,num=2)]\n",
    "max_features =['auto','sqrt']\n",
    "max_depth = [int(x) for x in range(10,25,5)]\n",
    "min_samples_split = [2,5,10]\n",
    "min_samples_leaf =[1,2,4]\n",
    "bootstrap =[True,False]\n",
    "\n",
    "# creat the random grid as dict type \n",
    "random_grid = {'n_estimators':n_estimators,\n",
    "              'max_features':max_features,\n",
    "              'min_samples_split':min_samples_split,\n",
    "              'min_samples_leaf':min_samples_leaf,\n",
    "              'bootstrap':bootstrap}#,\n",
    "#              'class_weight':'balanced'}\n",
    "rf = RandomForestClassifier(class_weight='balanced') # create a base model\n",
    "# initiate the random search model with only randonly select 50 combinations validate with 4 folds\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter =100, cv = 5, verbose =2, n_jobs=-1)\n",
    "rf_random.fit(train_x,train_y) # fit the model and find the best classifier\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "# calculating testing performance with the optimizer parameters\n",
    "\n",
    "# basde model \n",
    "clf_base = RandomForestClassifier(n_estimators =10, random_state =42)\n",
    "clf_base.fit(train_x,train_y)\n",
    "pred_labels = clf_base.predict(test_x)\n",
    "pred_probs = clf_base.predict_proba(test_x)[:,1]\n",
    "test_results = evaluate_classification(predictions= pred_labels, probs= pred_probs,test_labels=test_y)\n",
    "\n",
    "# Optimized model \n",
    "pred_labels = rf_random.best_estimator_.predict(test_x)\n",
    "pred_probs = rf_random.best_estimator_.predict_proba(test_x)[:,1]\n",
    "random_results = evaluate_classification(predictions= pred_labels, probs= pred_probs,test_labels=test_y)\n",
    "\n",
    "\n",
    "# some test scores\n",
    "results = {}    \n",
    "results['recall'] = recall_score(test_labels, predictions)\n",
    "results['precision'] = precision_score(test_labels, predictions)\n",
    "results['roc'] = roc_auc_score(test_labels, probs)\n",
    "\n",
    "# importance\n",
    "# impurity based\n",
    "importances = best_model.feature_importances_\n",
    "features = train_x.columns\n",
    "idx_sort = importances.argsort()[::-1] # sort the importance\n",
    "for i in range(len(idx_sort)):\n",
    "    print(\"%d. feature: %s (%.3f)\" % (i + 1, features[idx_sort[i]], importances[idx_sort[i]]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(np.arange(0,len(idx_sort)),importances[idx_sort])\n",
    "ax.set_yticklabels(features[idx_sort])\n",
    "ax.set_yticks(np.arange(0,len(idx_sort)))\n",
    "ax.set_title('Feature importance based on impurity')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# permutation based method after OHE /w seperated categorical features\n",
    "result = permutation_importance(best_model, test_x, test_y, n_repeats=10,\n",
    "                               random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "fig, ax= plt.subplots(figsize =(10,5))\n",
    "ax.boxplot(result.importances[sorted_idx].T,vert=False, labels = test_x.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the AU-ROC curve and confusion matrix for classification\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Plot formatting\n",
    "# plt.style.use('fivethirtyeight')\n",
    "# plt.rcParams['font.size'] = 18\n",
    "\n",
    "def evaluate_classification(predictions, probs,test_labels, train_predictions =[], train_probs =[],train_labels=[]):\n",
    "    \"\"\"Compare machine learning model to baseline performance.\n",
    "    Computes statistics and shows ROC curve.\n",
    "    - predictions: predicted labels\n",
    "    - probs: predicted probability\n",
    "    - test_labels: true labels for test data\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    baseline = {}\n",
    "    \n",
    "    baseline['recall'] = recall_score(test_labels, \n",
    "                                     [1 for _ in range(len(test_labels))])\n",
    "    baseline['precision'] = precision_score(test_labels, \n",
    "                                      [1 for _ in range(len(test_labels))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    results = {}    \n",
    "    results['recall'] = recall_score(test_labels, predictions)\n",
    "    results['precision'] = precision_score(test_labels, predictions)\n",
    "    results['roc'] = roc_auc_score(test_labels, probs)\n",
    "    \n",
    "    train_results = {}\n",
    "    if len(train_labels)!=0:\n",
    "        train_results['recall'] = recall_score(train_labels, train_predictions)\n",
    "        train_results['precision'] = precision_score(train_labels, train_predictions)\n",
    "        train_results['roc'] = roc_auc_score(train_labels, train_probs)\n",
    "\n",
    "    # print the results\n",
    "    for metric in ['recall', 'precision', 'roc']:\n",
    "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)}')\n",
    "    # print the trianing results\n",
    "    if len(train_labels)!=0:\n",
    "        for metric in ['recall', 'precision', 'roc']:\n",
    "            print(f'Train: {round(train_results[metric], 2)}')\n",
    "        \n",
    "        \n",
    "    # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(test_labels, probs)\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); \n",
    "    plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "    plt.show();\n",
    "    \n",
    "    return results, train_results\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    labelsize = 12\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 14)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = labelsize,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"red\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = labelsize)\n",
    "    plt.xlabel('Predicted label', size = labelsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H20 session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
